{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LluSLsTMh3gS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "# -----------------------------\n",
        "# Problem 1.1\n",
        "# -----------------------------\n",
        "\n",
        "def euclidean_dist(X_test, X_train):\n",
        "    dists = (\n",
        "        np.sum(X_test ** 2, axis=1, keepdims=True)\n",
        "        + np.sum(X_train ** 2, axis=1, keepdims=True).T\n",
        "        - 2 * X_test @ X_train.T\n",
        "    )\n",
        "    return dists\n",
        "\n",
        "\n",
        "def find_k_neighbors(dists, Y_train, k):\n",
        "    num_test = dists.shape[0]\n",
        "    neighbors = np.zeros((num_test, k))\n",
        "    sorted_idx = dists.argsort(axis=1)\n",
        "\n",
        "    for i in range(num_test):\n",
        "        neighbors[i] = Y_train[sorted_idx[i][:k]]\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def knn_predict(X_test, X_train, Y_train, k):\n",
        "    num_test = X_test.shape[0]\n",
        "    Y_pred = np.zeros(num_test, dtype=int)\n",
        "\n",
        "    dists = euclidean_dist(X_test, X_train)\n",
        "    neighbors = find_k_neighbors(dists, Y_train, k)\n",
        "\n",
        "    for i in range(num_test):\n",
        "        values, counts = np.unique(neighbors[i], return_counts=True)\n",
        "        Y_pred[i] = values[np.argmax(counts)]\n",
        "\n",
        "    return Y_pred\n",
        "\n",
        "\n",
        "def compute_error_rate(ypred, ytrue):\n",
        "    return (ypred != ytrue).mean() * 100\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Problem 1.2\n",
        "# -----------------------------\n",
        "\n",
        "def split_nfold(num_examples, n):\n",
        "    np.random.seed(1)\n",
        "\n",
        "    idx = np.random.permutation(num_examples).tolist()\n",
        "    fold_size = num_examples // n\n",
        "\n",
        "    train_sets = []\n",
        "    validation_sets = []\n",
        "\n",
        "    for i in range(n):\n",
        "        start = i * fold_size\n",
        "        end = (i + 1) * fold_size\n",
        "        if i == n - 1:\n",
        "            end = num_examples\n",
        "\n",
        "        val_set = idx[start:end]\n",
        "        train_set = idx[:start] + idx[end:]\n",
        "\n",
        "        train_sets.append(train_set)\n",
        "        validation_sets.append(val_set)\n",
        "\n",
        "    return train_sets, validation_sets\n",
        "\n",
        "\n",
        "def cross_validation(classifier, X, Y, n, *args):\n",
        "    np.random.seed(1)\n",
        "\n",
        "    errors = []\n",
        "    size = X.shape[0]\n",
        "\n",
        "    train_sets, val_sets = split_nfold(size, n)\n",
        "\n",
        "    for train_idx, val_idx in zip(train_sets, val_sets):\n",
        "        X_train = X[train_idx]\n",
        "        X_val = X[val_idx]\n",
        "        y_train = Y[train_idx]\n",
        "        y_val = Y[val_idx]\n",
        "\n",
        "        ypred = classifier(X_val, X_train, y_train, *args)\n",
        "        errors.append(compute_error_rate(ypred, y_val))\n",
        "\n",
        "    return np.mean(errors)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Problem 2\n",
        "# -----------------------------\n",
        "\n",
        "def problem2():\n",
        "    import os\n",
        "    import gzip\n",
        "\n",
        "\n",
        "    def maybe_download(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            from urllib.request import urlretrieve\n",
        "            urlretrieve(DATA_URL + filename, filename)\n",
        "\n",
        "    def load_images(filename):\n",
        "        maybe_download(filename)\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        return data.reshape(-1, 28 * 28) / np.float32(256)\n",
        "\n",
        "    def load_labels(filename):\n",
        "        maybe_download(filename)\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        return data\n",
        "\n",
        "    Xtrain = load_images('train-images-idx3-ubyte.gz')\n",
        "    ytrain = load_labels('train-labels-idx1-ubyte.gz')\n",
        "\n",
        "    size = 1000\n",
        "    k = 1\n",
        "\n",
        "    cvXtrain = Xtrain[:size]\n",
        "    cvytrain = ytrain[:size]\n",
        "\n",
        "    trial_folds = [3, 10, 50, 100, 1000]\n",
        "    cverror_rates = np.zeros(len(trial_folds))\n",
        "\n",
        "    for i, f in enumerate(trial_folds):\n",
        "        cverror_rates[i] = cross_validation(\n",
        "            knn_predict, cvXtrain, cvytrain, f, k\n",
        "        )\n",
        "\n",
        "    return cverror_rates\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Problem 3\n",
        "# -----------------------------\n",
        "\n",
        "def problem3():\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import classification_report\n",
        "\n",
        "    iris = load_iris()\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        iris.data, iris.target, test_size=0.2, random_state=1\n",
        "    )\n",
        "\n",
        "    # ---- Cross-validation to find best k ----\n",
        "    list_ks = np.arange(1, 101)\n",
        "    err_ks = np.zeros(len(list_ks))\n",
        "\n",
        "    for k in list_ks:\n",
        "        err_ks[k - 1] = cross_validation(knn_predict, X_train, Y_train, 10, k)\n",
        "\n",
        "    best_k = np.argmin(err_ks) + 1\n",
        "\n",
        "    # ---- Test set evaluation ----\n",
        "    y_pred = knn_predict(X_test, X_train, Y_train, best_k)\n",
        "    err_test = compute_error_rate(y_pred, Y_test)\n",
        "\n",
        "    # ---- Confusion Matrix ----\n",
        "    nclass = len(np.unique(Y_test))\n",
        "    cm = np.zeros((nclass, nclass), dtype=int)\n",
        "\n",
        "    for i in range(len(Y_test)):\n",
        "        cm[Y_test[i], y_pred[i]] += 1\n",
        "\n",
        "    # ---- Classification Report ----\n",
        "    cr = classification_report(Y_test, y_pred, output_dict=True)\n",
        "\n",
        "    # ---- F1 score per class (manual computation) ----\n",
        "    f1 = np.zeros(nclass)\n",
        "\n",
        "    for c in range(nclass):\n",
        "        tp = cm[c, c]\n",
        "        fp = cm[:, c].sum() - tp\n",
        "        fn = cm[c, :].sum() - tp\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "        f1[c] = (\n",
        "            2 * precision * recall / (precision + recall)\n",
        "            if (precision + recall) > 0 else 0\n",
        "        )\n",
        "\n",
        "\n",
        "    return err_ks, best_k, err_test, cm, cr, f1\n",
        "\n"
      ]
    }
  ]
}
